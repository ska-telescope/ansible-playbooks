.. doctest-skip-all
.. _kubespray-guide:

*****************************
Kubernetes cluster deployment
*****************************

This section describes requirements and guidelines.

Kubespray
=========

`Kubespray <https://kubespray.io/>`_ is an open-source, SIG community-created and -maintained collection of ansible playbooks, configuration files and externally contributed projects that facilitate the deployment of a production-level kubernetes cluster. The project supports multiple cloud providers, bare-metal deployments, operating systems and network layer implementations. Refer to their documentation for a deeper understanding of what you can and/or cannot do using this repository.

To create a kubernetes cluster one needs at least one Master and one Worker machine. We typically use Virtual Machines to accomplish this. These Virtual Machines have a number of prerequisites that need to be met, before you can start with installing kubernetes or configuring the cluster. These include updated packages, installed docker, disabled firewall and swap, and so on.

Prepare your machines
"""""""""""""""""""""

We have automated the setup steps mentioned above in a single `make` command that prepares your machines, installs kubernetes and configures your cluster. If you want to deploy a cluster, you need to prepare your machines as follows:

	#. All your machines should have internet access.
	#. You need to be able to SSH into all the machines. In our example command below, you need a private key named cloud.key in the root directory of the Ansible Playbook repository.
	#. Make sure that the machines can talk to each other. To test if this is possible, SSH into each node from the master. Here we only provide links to two example setups: Openstack and Virtualbox.

		3.1 If you use Virtualbox to create your machines, follow the instructions in `this Youtube video <https://www.youtube.com/watch?v=S7jD6nnYJy0/>`_.

		3.2 If your VMs were created in Openstack, be sure to assign them Floating IPs - refer to `this page in our developer portal <https://developer.skatelescope.org/en/latest/services/ait_performance_env.html/>`_ for details of how to do this.

.. note::
	Many writers suggest using a separate machine as Ansible host from which to deploy, especially when using *kubespray* - this is recommended if you are planning to use our makefile in an Openstack deployment as it is necessary that the host is on the same network as the nodes for deployment. Otherwise, you would need to configure the inventory group variables somewhat before running the ansible playbooks provided in the `kubespray` repository. Remember that the IP addresses used (see command below) will be the floating IPs.

Create cluster
""""""""""""""

In order to create a cluster with two machines, using two IP addresses 192.168.100.{25,26}, into which you can SSH with user `ubuntu` using the private key `~/cloud.key`, you only need to run `make k8s_cluster`.

Running `make help` will give output similar to this (truncated):

::

	$ make help
	make targets:
	Makefile:help                  show this help.
	Makefile:k8s                   Which kubernetes are we connected to
	....
	Makefile:vars                  Vagrant and DISPLAY variables
	k8s_cluster.mk:cluster         create a k8s cluster with ansible with IPs
	k8s_cluster.mk:reset           reset the k8s cluster with ansible with IPs

	make vars (+defaults):
	Makefile:DRIVER                true  ## Run Minikube via 'kvm2' driver (true) or 'none' (false)
	...
	Makefile:XAUTHORITYx           ${XAUTHORITY}
	k8s_cluster.mk:V_CLUSTERNAME   mycluster  ## Name of the cluster to be created
	k8s_cluster.mk:V_IPs           "192.168.100.25 192.168.100.26" ## Master node first, worker node after
	k8s_cluster.mk:V_PRIVATE_SSH_KEY ~/cloud.key  ## ssh private key to access the machines nodes of k8s
	k8s_cluster.mk:V_REMOTE_USER   ubuntu  ## remote username to access the machines nodes of k8s


Refer to the variables in *k8s_cluster.mk*. These are the defaults you want to change in order to set up your specific cluster.

This is an example of the `make cluster` command, for provisioning a three-machine cluster as user `jacksparrow` using the standard SSH private key generated by `ssh-keygen`:
::

	$ make cluster V_CLUSTERNAME=mycluster V_IPs="192.168.100.32 192.168.100.42 192.168.100.5" V_REMOTE_USER=jacksparrow V_PRIVATE_SSH_KEY=~/.ssh/id_rsa

There is also a `reset` target, which enables you to destroy the cluster. Note that you need the same parameters as before in order to access the machines:
::

	$ make reset V_CLUSTERNAME=mycluster V_IPs="192.168.100.32 192.168.100.42 192.168.100.5" V_REMOTE_USER=jacksparrow V_PRIVATE_SSH_KEY=~/.ssh/id_rsa

Cluster creation steps broken down
""""""""""""""""""""""""""""""""""

The `make cluster` target runs two Ansible Playbooks. The first is `k8s_create_inventory.yml`. It does not only create an inventory, but also prepares your machines for k8s installation. Steps like installing docker, disabling the firewalls and swap, and so on are taken care of. If you run the first ansible-playbook command under the cluster target you should find the kubespray repository downloaded at `/usr/src/kubespray`.

The second target simply calls kubespray's own cluster playbook, using the inventory that you created using the first. Under the kubespray directory there is a directory called `inventory`. In there you'll find `hosts.yaml`, and this is a file that you may want to edit if you have a solid understanding of what your cluster should look like, for instance in production.

Note that the default behaviour of the inventory creation step is to add two master nodes and to make all the nodes worker nodes. This is not necessarily best practice - you may want to isolate these cluster roles, and also the etcd nodes. A nine-machine cluster would for instance have three master nodes, three worker nodes and three etcd nodes. But this is all dependent on how you want your cluster set up.

.. todo::
    * Rewrite if we include *kubespray* as submodule.
    * Setting up AuthN/Z for your cluster.
    * Network provider (in our example cluster we deploy using calico)
	* SKA MVP cluster minimum machine requirements
