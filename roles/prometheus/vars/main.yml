---
# vars file for prometheus

node_exporter_version: 0.18.1-1.el7
prometheus_gitlab_ci_pipelines_exporter_tags: 0.3.1
prometheus_gitlab_ci_pipelines_exporter_token: ""

prometheus_binary_dir: /usr/local/bin
prometheus_config_dir: /etc/prometheus
prometheus_data_dir: /prometheus
prometheus_node_metric_relabel_configs: []
prometheus_static_node_metric_relabel_configs:
  - action: replace
    regex: 192\.168\.93\.201 :9100
    replacement: server3-2:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.203 :9100
    replacement: server2-1:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.211 :9100
    replacement: storage1-1:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.206 :9100
    replacement: server2-4:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.212 :9100
    replacement: storage1-2:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.204 :9100
    replacement: server2-2:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.202 :9100
    replacement: server3-3:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.208 :9100
    replacement: controller:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.209 :9100
    replacement: controller1-2:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.207 :9100
    replacement: server2-5:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.210 :9100
    replacement: storage2-1:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.200 :9100
    replacement: server3-1:9100
    source_labels:
    - instance
    target_label: instance
  - action: replace
    regex: 192\.168\.93\.205 :9100
    replacement: server2-3:9100
    source_labels:
    - instance
    target_label: instance

elasticsearch_instance_add_cluster_label:
  - regex: '^192\.168\.93\.(94|96|112):(\d+)'
    replacement: ska-systems1
    source_labels: [instance]
    target_label: cluster

prometheus_alertmanager_config_dir: /etc/alertmanager
prometheus_alertmanager_data_dir: /alertmanager

grafana_provisioning_config_dir: /etc/grafana/provisioning
grafana_data: /var/lib/grafana
grafana_dashboard_version: nautilus
grafana_datasource: Prometheus
grafana_dashboards_path: "{{ grafana_provisioning_config_dir }}/dashboards"
grafana_dashboard_files:
  - ceph-cluster.json
  - cephfs-overview.json
  - host-details.json
  - hosts-overview.json
  - osd-device-details.json
  - osds-overview.json
  - pool-detail.json
  - pool-overview.json
  - radosgw-detail.json
  - radosgw-overview.json
  - rbd-overview.json
grafana_plugins:
  - vonage-status-panel
  - grafana-piechart-panel
  - grafana-polystat-panel

# Alert Manager
# slack_api_url has to be set in the command line
slack_api_url: "*********************"

alerts_inhibit_rules: []
# - target_match:
#     severity: 'warning'
#   target_match:
#     severity: 'warning'
#   equal: ['Watchdog']
# - source_match:
#     severity: 'critical'
#     instance: '192.168.93.205:9100'
#   target_match:
#     severity: 'warning'
# - source_match:
#     severity: 'critical'
#     instance: '192.168.93.206:9100'
#   target_match:
#     severity: 'warning'

# can only do one cluster - v1
k8s_api_server_addr: "192.168.93.102"
k8s_api_server_port: "6443"
k8s_api_server: "https://{{ k8s_api_server_addr }}:{{ k8s_api_server_port }}"
k8s_certificate_authority: ""
k8s_client_certificate: ""
k8s_client_key: ""
k8s_bearer_token: "/etc/prometheus/bearer.token"
k8s_username: ""

# k8s_api_server: ""
# k8s_certificate_authority: "/home/ubuntu/.minikube/ca.crt"
# k8s_client_certificate: "/home/ubuntu/.minikube/client.crt"
# k8s_client_key: "/home/ubuntu/.minikube/client.key"
# k8s_bearer_token: ""
# k8s_username: minikube

playbook_dir: /usr/src/ansible-playbooks/

auth_url: "http://192.168.93.215:5000/v3/"
username: "matteodicarlo"
password: "*************"
project_name: "geral;system-team;admin"

prometheus_global:
  scrape_interval: 15s
  scrape_timeout: 10s
  evaluation_interval: 15s

prometheus_remote_write: []

prometheus_remote_read: []

prometheus_alertmanager_config:
  - scheme: http
    static_configs:
      - targets: ["{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:9093"]

prometheus_scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets:
          - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:9090"
          - "192.168.93.208:9090" #controller1-1
          - "192.168.93.209:9090" #controller1-2

  - job_name: "gitlab_ci_pipelines_exporter"
    static_configs:
      - targets:
          - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:8080"

  - job_name: "node"
    static_configs:
    - targets:
          - "192.168.93.12:9100" #nexus
          - "192.168.93.200:9100" #server3-1
          - "192.168.93.201:9100" #server3-2
          - "192.168.93.202:9100" #server3-3
          - "192.168.93.203:9100" #server2-1
          - "192.168.93.204:9100" #server2-2
          - "192.168.93.205:9100" #server2-3
          - "192.168.93.206:9100" #server2-4
          - "192.168.93.207:9100" #server2-5
          - "192.168.93.208:9100" #controller1-1
          - "192.168.93.209:9100" #controller1-2
          - "192.168.93.210:9100" #storage2-1
          - "192.168.93.211:9100" #storage1-1
          - "192.168.93.212:9100" #storage1-2
    file_sd_configs:
    - files:
      - 'node_exporter.json'
    metric_relabel_configs: "{{ prometheus_static_node_metric_relabel_configs + elasticsearch_instance_add_cluster_label + prometheus_node_metric_relabel_configs }}"

  - job_name: "gitlab-runner"
    file_sd_configs:
    - files:
      - 'gitlab_exporter.json'
    metric_relabel_configs: "{{ prometheus_static_node_metric_relabel_configs + elasticsearch_instance_add_cluster_label + prometheus_node_metric_relabel_configs }}"

  - job_name: "elasticsearch"
    file_sd_configs:
    - files:
      - 'elasticsearch_exporter.json'

  - job_name: "docker"
    file_sd_configs:
    - files:
      - 'docker_exporter.json'

  - job_name: "docker_cadvisor"
    file_sd_configs:
    - files:
      - 'docker_cadvisor.json'
    metric_relabel_configs: "{{ prometheus_static_node_metric_relabel_configs + elasticsearch_instance_add_cluster_label + prometheus_node_metric_relabel_configs }}"

  - job_name: "kube-state-metrics"
    file_sd_configs:
    - files:
      - 'kubernetes_exporter.json'

  - job_name: "k8stelemetry"
    file_sd_configs:
    - files:
      - 'kubernetes_telemetry.json'

  - job_name: "ceph_cluster"
    file_sd_configs:
    - files:
      - 'ceph-mgr.json'

blackbox_ssh_targets:
  - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:22" #matteo devenv
  - "192.168.93.37:22" #ssh skampi pipeline cluster node1
  - "192.168.93.40:22" #ssh skampi pipeline cluster node2
  - "192.168.93.74:22" #ssh skampi pipeline cluster node3
  - "192.168.93.38:22" #ssh skampi pipeline cluster node4

prometheus_record_rules:
  - record: instance:node_cpu:load
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

  - record: instance:node_ram:usage
    expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100

  - record: instance:node_fs:disk_space
    expr: node_filesystem_avail_bytes{mountpoint="/",job="node"} / node_filesystem_size_bytes{mountpoint="/",job="node"} * 100

prometheus_alert_rules:
- alert: Watchdog
  expr: vector(1)
  for: 10m
  labels:
    severity: warning
  annotations:
    description: 'This is an alert meant to ensure that the entire alerting pipeline is functional.
      This alert is always firing, therefore it should always be firing in Alertmanager
      and always fire against a receiver. There are integrations with various notification
      mechanisms that send a notification when this alert is not firing. For example the
      "DeadMansSnitch" integration in PagerDuty.'
    summary: 'Ensure entire alerting pipeline is functional'
- alert: InstanceDown
  expr: "up == 0"
  for: 5m
  labels:
    severity: critical
  annotations:
    description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} down{% endraw %}"
- alert: SshServiceDown
  expr: "probe_success == 0"
  for: 5m
  labels:
    severity: critical
  annotations:
    description: "{% raw %}{{ $labels.target }} of job {{ $labels.job }} has been down for more than 5 minutes.{% endraw %}"
    summary: "{% raw %}Ssh Service {{ $labels.target }} down{% endraw %}"
- alert: CriticalCPULoad
  expr: 'instance:node_cpu:load > 98'
  for: 30m
  labels:
    severity: critical
  annotations:
    description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has Critical CPU load for more than 30 minutes.{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} - Critical CPU load{% endraw %}"
- alert: CriticalRAMUsage
  expr: 'instance:node_ram:usage > 98'
  for: 5m
  labels:
    severity: critical
  annotations:
    description: "{% raw %}{{ $labels.instance }} has Critical Memory Usage more than 5 minutes.{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} has Critical Memory Usage{% endraw %}"
- alert: CriticalDiskSpace
  expr: 'instance:node_fs:disk_space < 20'
  for: 4m
  labels:
    severity: critical
  annotations:
    description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has less than 10% space remaining.{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} - Critical disk space usage{% endraw %}"
- alert: ClockSkewDetected
  expr: 'abs(node_timex_offset_seconds) * 1000 > 30'
  for: 2m
  labels:
    severity: warning
  annotations:
    description: "{% raw %}Clock skew detected on {{ $labels.instance }}. Ensure NTP is configured correctly on this host.{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} - Clock skew detected{% endraw %}"
## Disable as Skampi containers continually restart due to unsolved errors - PXH
# - alert: NumberRestartPerContainer
#   expr: 'kube_pod_container_status_restarts_total > 30'
#   for: 1m
#   labels:
#     severity: critical
#   annotations:
#     description: "{% raw %} Container {{ $labels.container }} of pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} keeps restarting.{% endraw %}"
#     summary: "{% raw %}Container {{ $labels.container }} - Too many restarts{% endraw %}"
- alert: NumberRestartPerInitContainerContainer
  expr: 'kube_pod_init_container_status_restarts_total > 10'
  for: 1m
  labels:
    severity: critical
  annotations:
    description: "{% raw %} Container {{ $labels.container }} of pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} keeps restarting.{% endraw %}"
    summary: "{% raw %}Container {{ $labels.container }} - Too many restarts{% endraw %}"
- alert: PendingJobs
  expr: 'gitlab_runner_jobs{state="pending"} > 0'
  for: 10m
  labels:
    severity: critical
  annotations:
    description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has pending jobs for more than 5 minutes..{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} - Gitlab runner pending jobs{% endraw %}"
