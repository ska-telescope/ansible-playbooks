---
# vars file for prometheus

prometheus_binary_dir: /usr/local/bin
prometheus_config_dir: /etc/prometheus
prometheus_data_dir: /prometheus

prometheus_alertmanager_config_dir: /etc/alertmanager
prometheus_alertmanager_data_dir: /alertmanager

slack_api_url: "https://hooks.slack.com/services/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"

prometheus_global:
  scrape_interval: 15s
  scrape_timeout: 10s
  evaluation_interval: 15s

prometheus_remote_write: []

prometheus_remote_read: []

prometheus_alertmanager_config:
  - scheme: http
    static_configs:
      - targets: ["{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:9093"]

prometheus_scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets:
          - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:9090"

  - job_name: "node"
    static_configs:
      - targets:
          - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:9100"

prometheus_record_rules:
  # The count of CPUs per node, useful for getting CPU time as a percent of total.
  - record: instance:node_cpus:count
    expr: count(node_cpu{mode="idle"}) without (cpu,mode)

  # CPU in use by CPU.
  - record: instance_cpu:node_cpu_not_idle:rate5m
    expr: sum(rate(node_cpu{mode!="idle"}[5m])) without (mode)

  # CPU in use by mode.
  - record: instance_mode:node_cpu:rate5m
    expr: sum(rate(node_cpu[5m])) without (cpu)

  # CPU in use ratio.
  - record: instance:node_cpu_utilization:ratio
    expr: sum(rate(node_cpu{mode!="idle"}[5m])) without (mode) / count(node_cpu)

prometheus_alert_rules:
- alert: Watchdog
  expr: vector(1)
  for: 10m
  labels:
    severity: warning
  annotations:
    description: 'This is an alert meant to ensure that the entire alerting pipeline is functional.
      This alert is always firing, therefore it should always be firing in Alertmanager
      and always fire against a receiver. There are integrations with various notification
      mechanisms that send a notification when this alert is not firing. For example the
      "DeadMansSnitch" integration in PagerDuty.'
    summary: 'Ensure entire alerting pipeline is functional'
- alert: InstanceDown
  expr: "up == 0"
  for: 5m
  labels:
    severity: critical
  annotations:
    description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} down{% endraw %}"
- alert: CriticalCPULoad
  expr: '100 - (avg by(instance) (irate(node_cpu{mode="idle"}[5m])) * 100) > 80'
  for: 2m
  labels:
    severity: critical
  annotations:
    description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has Critical CPU load for more than 2 minutes.{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} - Critical CPU load{% endraw %}"
- alert: CriticalRAMUsage
  expr: '(1 - node_memory_MemAvailable / node_memory_MemTotal) * 100 > 98'
  for: 5m
  labels:
    severity: critical
  annotations:
    description: "{% raw %}{{ $labels.instance }} has Critical Memory Usage more than 5 minutes.{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} has Critical Memory Usage{% endraw %}"
- alert: CriticalDiskSpace
  expr: 'node_filesystem_avail{mountpoint="/",job="node"} / node_filesystem_size{mountpoint="/",job="node"} < 0.2'
  for: 4m
  labels:
    severity: critical
  annotations:
    description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has less than 10% space remaining.{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} - Critical disk space usage{% endraw %}"
- alert: ClockSkewDetected
  expr: 'abs(node_timex_offset_seconds) * 1000 > 30'
  for: 2m
  labels:
    severity: warning
  annotations:
    description: "{% raw %}Clock skew detected on {{ $labels.instance }}. Ensure NTP is configured correctly on this host.{% endraw %}"
    summary: "{% raw %}Instance {{ $labels.instance }} - Clock skew detected{% endraw %}"
